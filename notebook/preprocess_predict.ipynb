{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import feather\n",
    "import datetime\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "pd.set_option('display.max_Columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8359, 16)\n",
      "(8360, 11)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../inputs/train.csv')\n",
    "print(train.shape)\n",
    "test = pd.read_csv('../inputs/test.csv')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creansing(df_input):\n",
    "    df_input['User_Score_tbd'] = (df_input['User_Score'] == 'tbd').astype(int)\n",
    "    df_input['User_Score'] = df_input['User_Score'].replace({'nan': np.NaN, 'tbd': np.NaN}).astype(float)\n",
    "    df_input['User_Score_int'] = df_input['User_Score'].round()\n",
    "    df_input['Critic_Score_int'] = df_input['Critic_Score'].round(-1)/10\n",
    "    df_input['User_Count_log'] = df_input['User_Count'].apply(np.log1p)\n",
    "    df_input['Year'] = df_input['Year_of_Release'].fillna(0).round().astype(int)\n",
    "    df_input.loc[(df_input['Year']>0) & (df_input['Year']<1995), 'Year'] = 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creansing(train)\n",
    "creansing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_overlapping(train: pd.DataFrame, test: pd.DataFrame, column: str):\n",
    "    \"\"\"train/testにしか出てこない値を調べる\"\"\"\n",
    "    only_in_train = set(train[column].unique()) - set(test[column].unique())\n",
    "    only_in_test = set(test[column].unique()) - set(train[column].unique())\n",
    "    non_overlapping = only_in_train.union(only_in_test)\n",
    "    return non_overlapping\n",
    "\n",
    "def category2num(train: pd.DataFrame, test: pd.DataFrame, columns: list):\n",
    "    train_ = train.copy()\n",
    "    test_ = test.copy()\n",
    "    for column in columns:\n",
    "        non_overlapping = get_non_overlapping(train, test, column)\n",
    "        try:\n",
    "            if train[column].dtype == np.dtype(\"O\"):\n",
    "                # dtypeがobjectなら欠損は'missing' クラスにする\n",
    "                train_[column] = train[column].fillna(\"missing\")\n",
    "                test_[column] = test[column].fillna(\"missing\")\n",
    "                train_[column] = train_[column].map(lambda x: x if x not in non_overlapping else \"other\")\n",
    "                test_[column] = test_[column].map(lambda x: x if x not in non_overlapping else \"other\")\n",
    "            else:\n",
    "                # dtypeがint/floatなら欠損は'-1'とする\n",
    "                train_[column] = train[column].fillna(-1)\n",
    "                test_[column] = test[column].fillna(-1)\n",
    "                train_[column] = train_[column].map(lambda x: x if x not in non_overlapping else -2)\n",
    "                test_[column] = test_[column].map(lambda x: x if x not in non_overlapping else -2)\n",
    "\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            concatenated = pd.concat([train_, test_], axis=0).reset_index(drop=True)\n",
    "            le.fit(concatenated[column])\n",
    "            train_[column] = le.transform(train_[column])\n",
    "            test_[column] = le.transform(test_[column])\n",
    "        except Exception:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoding(df_train, df_test):\n",
    "    tr, te = category2num(df_train, df_test, ['Platform', 'Genre', 'Developer', 'Rating'])\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encoding(df_train, df_test, col):\n",
    "    tr = df_train[col]\n",
    "    te = df_test[col]\n",
    "    whole = pd.concat([tr, te])\n",
    "    vc = whole.value_counts(dropna=False)\n",
    "    tr_ = pd.DataFrame(tr.map(vc)).add_prefix('CE_')\n",
    "    te_ = pd.DataFrame(te.map(vc)).add_prefix('CE_')\n",
    "    return tr_, te_\n",
    "\n",
    "def create_count_encoding_feature(df_train, df_test):\n",
    "    tr = pd.DataFrame()\n",
    "    te = pd.DataFrame()\n",
    "    for col in ['Platform', 'Genre', 'Publisher', 'Developer', 'Rating', 'User_Score_int', 'Critic_Score_int']:\n",
    "        tr_, te_ = count_encoding(df_train, df_test, col)\n",
    "        tr = pd.concat([tr, tr_], axis=1)\n",
    "        te = pd.concat([te, te_], axis=1)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(src, dst, group_col, target_col, aggfunc):\n",
    "    agg_dict = {}\n",
    "    for tc in target_col:\n",
    "        agg_dict[tc] = aggfunc\n",
    "    target = src.groupby(group_col).agg(agg_dict).reset_index()\n",
    "    \n",
    "    merge = pd.merge(dst[group_col], target, on=group_col, how='left').set_index(dst.index)\n",
    "    suffix = '_'.join(group_col)\n",
    "    return merge.drop(columns=group_col).add_prefix('TGE_').add_suffix('_{}_by_{}'.format(aggfunc, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crete_target_enc_train_test(df_train, df_test, group_col, target_col, aggfunc):\n",
    "    target = pd.DataFrame()\n",
    "    kf = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "    for train_index, eval_index in kf.split(df_train):\n",
    "        kf_train = df_train.iloc[train_index]\n",
    "        kf_eval = df_train.iloc[eval_index]\n",
    "        tmp = target_encoding(kf_train, kf_eval, group_col, target_col, aggfunc)\n",
    "        target = pd.concat([target, tmp])\n",
    "    # train\n",
    "    tr = target.sort_index()\n",
    "    # test\n",
    "    te = target_encoding(df_train, df_test, group_col, target_col, aggfunc)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoding_sub(df_train, df_test, tr, te, group, target, aggfunc):\n",
    "    tr_, te_ = crete_target_enc_train_test(df_train, df_test, group, target, aggfunc)\n",
    "    tr = pd.concat([tr, tr_], axis=1)\n",
    "    te = pd.concat([te, te_], axis=1)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoding(df_train, df_test):\n",
    "    tr = pd.DataFrame()\n",
    "    te = pd.DataFrame()\n",
    "#     target = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "    target = ['Global_Sales']\n",
    "    groups = [\n",
    "        ['Platform'],\n",
    "        ['Genre'],\n",
    "        ['Rating'],\n",
    "        ['User_Score_int'],\n",
    "        ['Critic_Score_int'],\n",
    "        \n",
    "        ['Platform', 'Genre'],\n",
    "        ['Platform', 'Rating'],\n",
    "        ['Genre', 'Rating'],\n",
    "        \n",
    "        ['User_Score_int', 'Platform'],\n",
    "        ['User_Score_int', 'Genre'],\n",
    "        ['User_Score_int', 'Rating'],\n",
    "        ['Critic_Score_int', 'Platform'],\n",
    "        ['Critic_Score_int', 'Genre'],\n",
    "        ['Critic_Score_int', 'Rating'],\n",
    "        ['Critic_Score_int', 'User_Score_int'],\n",
    "    ]\n",
    "    for gr in groups:\n",
    "        tr, te = create_target_encoding_sub(df_train, df_test, tr, te, gr, target, 'mean')\n",
    "    for gr in groups:\n",
    "        tr, te = create_target_encoding_sub(df_train, df_test, tr, te, gr, target, 'std')\n",
    "    \n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word (TruncatedSVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pivot(df_input, wordlist):\n",
    "    pivot = pd.DataFrame(index=df_input.index, columns=wordlist)\n",
    "    pivot = pd.concat([df_input['Name'].str.lower(), pivot], axis=1)\n",
    "    for c in pivot.columns:\n",
    "        if c == 'Name':\n",
    "            continue\n",
    "        pivot[c] = pivot['Name'].str.contains(c)\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist():\n",
    "    trainName = train['Name'].str.lower()\n",
    "    testName = test['Name'].str.lower()\n",
    "    name = pd.concat([trainName, testName]).reset_index(drop=True)\n",
    "    clean_name = name.str.replace('[^0-9A-Za-z]', ' ')\n",
    "    \n",
    "    wordlist = []\n",
    "    for n in list(clean_name):\n",
    "        wordlist = wordlist + (str(n).split())\n",
    "    print(len(wordlist))\n",
    "    \n",
    "    df_name = pd.DataFrame(pd.DataFrame(wordlist, columns=['name']).value_counts()).reset_index()\n",
    "    df_name.rename(columns={0:'name_count'}, inplace=True)\n",
    "    \n",
    "    # 3文字以上、3回以上出現している単語\n",
    "    df_name['word_len'] = df_name['name'].str.len()\n",
    "    df_longname = df_name[df_name['word_len'] > 3]\n",
    "    df_longname = df_longname[df_longname['name_count'] > 3]\n",
    "    \n",
    "    # 含まれるかどうか\n",
    "    train_pivot = word_pivot(train, list(df_longname['name']))\n",
    "    print('train_pivot', train_pivot.shape)\n",
    "    test_pivot = word_pivot(test, list(df_longname['name']))\n",
    "    print('test_pivot', test_pivot.shape)\n",
    "    \n",
    "    # trainとtestの両方に存在するものだけ(train>3, test>0)\n",
    "    train_sum = pd.DataFrame(train_pivot.drop(columns=['Name']).sum(), columns=['tr_count'])\n",
    "    test_sum = pd.DataFrame(test_pivot.drop(columns=['Name']).sum(), columns=['te_count'])\n",
    "    train_test_sum = pd.concat([train_sum, test_sum], axis=1).reset_index()\n",
    "    new_wordlist = train_test_sum[(train_test_sum['tr_count']>3)&(train_test_sum['te_count']>0)].reset_index(drop=True)\n",
    "    \n",
    "    train_pivot2 = word_pivot(train, list(new_wordlist['index']))\n",
    "    print('train_pivot2', train_pivot2.shape)\n",
    "    test_pivot2 = word_pivot(test, list(new_wordlist['index']))\n",
    "    print('test_pivot2', test_pivot2.shape)\n",
    "    \n",
    "    train_pivot2.to_feather('../inputs/train_word.f')\n",
    "    test_pivot2.to_feather('../inputs/test_word.f')\n",
    "    \n",
    "    return train_pivot2, test_pivot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_create_wordlist = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_features(df_train, df_test):\n",
    "    \n",
    "    if skip_create_wordlist:\n",
    "        train_word = pd.read_feather('../inputs/train_word.f')\n",
    "        test_word = pd.read_feather('../inputs/test_word.f')\n",
    "    else:\n",
    "        train_word, test_word = create_wordlist()\n",
    "        \n",
    "    wd_tr = train_word.drop(columns=['Name']).fillna(0).astype(int).add_prefix('WD_')\n",
    "    wd_te = test_word.drop(columns=['Name']).fillna(0).astype(int).add_prefix('WD_')\n",
    "    print('wd_tr', wd_tr.shape)\n",
    "    print('wd_te', wd_te.shape)\n",
    "    \n",
    "    return wd_tr, wd_te\n",
    "\n",
    "#     ntopic = 5\n",
    "#     lda = LatentDirichletAllocation(n_components=ntopic)\n",
    "#     x_tr = pd.DataFrame(lda.fit_transform(wd_tr)).add_prefix('LDA_')\n",
    "#     x_te = pd.DataFrame(lda.fit_transform(wd_te)).add_prefix('LDA_')\n",
    "    \n",
    "#     svd = TruncatedSVD(5)\n",
    "#     x_tr = pd.DataFrame(svd.fit_transform(wd_tr)).add_prefix('SVD_')\n",
    "#     x_te = pd.DataFrame(svd.fit_transform(wd_te)).add_prefix('SVD_')\n",
    "#     return x_tr, x_te\n",
    "    \n",
    "#     tr_ = pd.concat([wd_tr, x_tr], axis=1)\n",
    "#     te_ = pd.concat([wd_te, x_te], axis=1)\n",
    "#     return tr_, te_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_topic(df_input, ntopic, index, column):\n",
    "\n",
    "    df_input['dummy'] = 1\n",
    "    pvt_tbl = pd.pivot_table(df_input, index=index, columns=column, values='dummy', aggfunc='count')\n",
    "    pvt_tbl.fillna(0, inplace=True)\n",
    "    \n",
    "    prefix = 'LDA_{}_'.format(column)\n",
    "    suffix = '_by_{}'.format(index)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=ntopic)\n",
    "    lda_out = pd.DataFrame(lda.fit_transform(pvt_tbl), index=pvt_tbl.index).add_prefix(prefix).add_suffix(suffix)\n",
    "    return lda_out.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_features(df_train, df_test):\n",
    "    cat_list = ['Platform', 'Genre', 'Developer', 'Rating', 'Publisher']\n",
    "    tr = df_train[cat_list].copy()\n",
    "    te = df_test[cat_list].copy()\n",
    "    whole = pd.concat([tr, te])\n",
    "    \n",
    "    for a, b in itertools.permutations(cat_list, 2):\n",
    "        lda_o = LDA_topic(whole, 5, index=a, column=b)\n",
    "        tr = pd.merge(tr, lda_o, on=a, how='left')\n",
    "        te = pd.merge(te, lda_o, on=a, how='left')\n",
    "        \n",
    "    return tr.drop(columns=cat_list), te.drop(columns=cat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### salesの割合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_columns = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']\n",
    "\n",
    "def sales_groupby(df_train, df_test, groupkey):\n",
    "    tr = df_train[groupkey].copy()\n",
    "    te = df_test[groupkey].copy()\n",
    "    grp_sum = df_train[sales_columns].groupby(df_train[groupkey]).sum()\n",
    "    \n",
    "    # 列方向の割合\n",
    "    salescol_sum = grp_sum.sum()\n",
    "    for c in sales_columns:\n",
    "        grp_sum['Rate1_'+c] = grp_sum[c] / salescol_sum[c]\n",
    "    grp_sum.drop(columns=['Rate1_Global_Sales'], inplace=True)\n",
    "        \n",
    "    # 行方向の割合\n",
    "#     for c in sales_columns:\n",
    "#         grp_sum['Rate2_'+c] = grp_sum[c] / grp_sum['Global_Sales']\n",
    "#     grp_sum.drop(columns=['Rate2_Global_Sales'], inplace=True)\n",
    "    \n",
    "    grp_sum.drop(columns=sales_columns, inplace=True)\n",
    "    grp_sum = grp_sum.add_suffix('_by_{}'.format(groupkey))\n",
    "\n",
    "    tr = pd.merge(tr, grp_sum.reset_index(), on=groupkey, how='left').drop(columns=groupkey)\n",
    "    te = pd.merge(te, grp_sum.reset_index(), on=groupkey, how='left').drop(columns=groupkey)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sales_feature(df_train, df_test):\n",
    "    tr = pd.DataFrame(index=df_train.index)\n",
    "    te = pd.DataFrame(index=df_test.index)\n",
    "    keys = ['Platform', 'Genre', 'Developer', 'Rating', 'Year']\n",
    "    for k in keys:\n",
    "        tr_, te_ = sales_groupby(train, test, k)\n",
    "        tr = pd.concat([tr, tr_], axis=1)\n",
    "        te = pd.concat([te, te_], axis=1)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_publisher_feature(df_train, df_test):\n",
    "    df_whole = pd.concat([df_train, df_test])\n",
    "    plat_pivot = df_whole.pivot_table(index='Publisher', columns='Platform', values='Name', aggfunc='count')\n",
    "    genre_pivot = df_whole.pivot_table(index='Publisher', columns='Genre', values='Name', aggfunc='count')\n",
    "    year_pivot = df_whole.pivot_table(index='Publisher', columns='Year', values='Name', aggfunc='count')\n",
    "    print('plat_pivot', len(plat_pivot))\n",
    "    print('genre_pivot', len(genre_pivot))\n",
    "    print('year_pivot', len(year_pivot))\n",
    "    \n",
    "    all_pivot = pd.concat([plat_pivot, genre_pivot], axis=1)\n",
    "    all_pivot = pd.concat([all_pivot, year_pivot], axis=1)\n",
    "    all_pivot.fillna(0, inplace=True)\n",
    "    print('all_pivot', len(all_pivot))\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=10)\n",
    "    lda_out = pd.DataFrame(lda.fit_transform(all_pivot), index=all_pivot.index).add_prefix('LDA_Publisher_').reset_index()\n",
    "    \n",
    "    tr = pd.merge(df_train[['Publisher']], lda_out, on='Publisher', how='left').drop(columns=['Publisher'])\n",
    "    te = pd.merge(df_test[['Publisher']], lda_out, on='Publisher', how='left').drop(columns=['Publisher'])\n",
    "    print(tr.shape)\n",
    "    print(te.shape)\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = [\n",
    "#     create_sales_feature,\n",
    "    create_count_encoding_feature,\n",
    "    create_target_encoding,\n",
    "    create_lda_features,\n",
    "    create_word_features,\n",
    "    create_publisher_feature,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(train, test):\n",
    "    train_cp = train.copy()\n",
    "    test_cp = test.copy()\n",
    "    \n",
    "    for func in processors:\n",
    "        tr, te = func(train_cp, test_cp)\n",
    "        train_cp = pd.concat([train_cp, tr], axis=1)\n",
    "        test_cp = pd.concat([test_cp, te], axis=1)\n",
    "\n",
    "    # label enc\n",
    "#     train_cp, test_cp = create_label_encoding(train_cp, test_cp)\n",
    "    \n",
    "    print('train', train_cp.shape)\n",
    "    print('test', test_cp.shape)\n",
    "    assert len(train) == len(train_cp)\n",
    "    assert len(test) == len(test_cp)\n",
    "    return train_cp, test_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_tr (8359, 1271)\n",
      "wd_te (8360, 1271)\n",
      "plat_pivot 581\n",
      "genre_pivot 581\n",
      "year_pivot 581\n",
      "all_pivot 581\n",
      "(8359, 10)\n",
      "(8360, 10)\n",
      "train (8359, 1439)\n",
      "test (8360, 1434)\n"
     ]
    }
   ],
   "source": [
    "train_, test_ = create_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.to_feather('../inputs/train_cat_feature.f')\n",
    "test_.to_feather('../inputs/test_cat_feature.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label enc\n",
    "train_, test_ = create_label_encoding(train_, test_)\n",
    "train_.to_feather('../inputs/train_feature.f')\n",
    "test_.to_feather('../inputs/test_feature.f')\n",
    "\n",
    "assert len(train) == len(train_)\n",
    "assert len(test) == len(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22\n",
    "- PublisherごとのcountをLDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
